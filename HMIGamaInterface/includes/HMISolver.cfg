verbose = true
logPath = log
overwriteExistingLogFiles = false
logFilePostfix =
saveParticles = true

[plugins]
heuristicPlugin = libhmiHeuristicPlugin.so

planningRewardPlugin = libhmiRewardPlugin.so
executionRewardPlugin = libhmiRewardPlugin.so

planningTerminalPlugin = libhmiTerminalPlugin.so
executionTerminalPlugin = libhmiTerminalPlugin.so

planningTransitionPlugin = libhmiTransitionPlugin.so
executionTransitionPlugin = libhmiTransitionExecutionPlugin.so

planningObservationPlugin = libhmiObservationPlugin.so
executionObservationPlugin = libhmiObservationExecutionPlugin.so

planningInitialBeliefPlugin = libhmiInitialBeliefPlugin.so
executionInitialBeliefPlugin = libhmiInitialBeliefPlugin.so

[transitionPluginOptions]
gridPath = ../models/HMIModel/HMIGrid.txt
transitionMatrixPath = ../models/HMIModel/HMITransitionMatrix.txt
randomAgentsPath = ../models/HMIModel/HMIRandomAgents.txt

[transitionExecutionPluginOptions]
randomAgentsPath = ../models/HMIModel/HMIRandomAgents.txt
pipePathToGama = ../pipes/pipeToGama
pipePathToSolver = ../pipes/statePipeToSolver

[terminalPluginOptions]
gridPath = ../models/HMIModel/HMIGrid.txt
randomAgentsPath = ../models/HMIModel/HMIRandomAgents.txt

[rewardPluginOptions]
gridPath = ../models/HMIModel/HMIGrid.txt
randomAgentsPath = ../models/HMIModel/HMIRandomAgents.txt

[observationPluginOptions]
gridPath = ../models/HMIModel/HMIGrid.txt
transitionMatrixPath = ../models/HMIModel/HMITransitionMatrix.txt
randomAgentsPath = ../models/HMIModel/HMIRandomAgents.txt

[observationExecutionPluginOptions]
randomAgentsPath = ../models/HMIModel/HMIRandomAgents.txt
pipePathToGama = ../pipes/pipeToGama
pipePathToSolver = ../pipes/observationPipeToSolver

[initialBeliefOptions]
gridPath = ../models/HMIModel/HMIGrid.txt
initialRobotState =
initialRandomAgentState =

[heuristicPluginOptions]
gridPath = ../models/HMIModel/HMIGrid.txt
transitionMatrixPath = ../models/HMIModel/HMITransitionMatrix.txt
randomAgentsPath = ../models/HMIModel/HMIRandomAgents.txt

[problem]

# Number of simulation runs
nRuns = 3

# Maximum number of steps to reach the goal - TODO not sure how this applies to our infinite horizon problem
nSteps = 10

# The planning environment path
planningEnvironmentPath = 

# The execution environment path
executionEnvironmentPath = 

# Do not need a robot SDF model for this problem, hence no robotName = x
robotName = RocksampleRobot

enableGazeboStateLogging = false

# The discount factor of the reward model
discountFactor = 0.95

# The maximum time to spend on each step, in milliseconds (0 => no time limit)
stepTimeout = 1000

##################################################################################################
##################################################################################################
#### State, action and observation description
##################################################################################################
##################################################################################################

[state]

[action]

[observation]

[changes]
hasChanges = false
changesPath = changes_1.txt
areDynamic = false

[ABT]
# The number of trajectories to simulate per time step (0 => wait for timeout)
historiesPerStep = 0

# If this is set to "true", ABT will prune the tree after every step.
pruneEveryStep = true

# The particle filter to use
particleFilter = observationModel

# The minimum number of particles for the current belief state in a simulation.
# Extra particles will be resampled via a particle filter if the particle count
# for the *current* belief state drops below this number during simulation.
minParticleCount = 10000

# The maximum depth of the search tree 
maximumDepth = 3

# True if the above maximum depth is relative to the initial belief, and false
# if it is relative to the current belief.
isAbsoluteHorizon = false

# Here we use UCB1 as the action selection strategy with an exploration constant of 5.0
searchStrategy = ucb(5.0)

#savePolicy = false
#loadInitialPolicy = false
#policyPath = final-0.pol

actionType = discrete
numInputStepsActions =

observationType = discrete
numInputStepsObservations =

# If this is set to true, the policy will be recomputed from scratch after every step 
resetPolicy = false

[simulation]
interactive = false
particlePlotLimit = 0